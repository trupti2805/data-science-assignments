{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dee69f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation,Layer,Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5ece90f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "forestfires=pd.read_csv(\"C:\\\\Users\\\\Trupti Kendre\\\\Downloads\\\\forestfires (1).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f618c1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "forestfires.drop([\"month\",\"day\"],axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0c2c24fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "small    378\n",
       "large    139\n",
       "Name: size_category, dtype: int64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forestfires[\"size_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "987bce46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FFMC             0\n",
       "DMC              0\n",
       "DC               0\n",
       "ISI              0\n",
       "temp             0\n",
       "RH               0\n",
       "wind             0\n",
       "rain             0\n",
       "area             0\n",
       "dayfri           0\n",
       "daymon           0\n",
       "daysat           0\n",
       "daysun           0\n",
       "daythu           0\n",
       "daytue           0\n",
       "daywed           0\n",
       "monthapr         0\n",
       "monthaug         0\n",
       "monthdec         0\n",
       "monthfeb         0\n",
       "monthjan         0\n",
       "monthjul         0\n",
       "monthjun         0\n",
       "monthmar         0\n",
       "monthmay         0\n",
       "monthnov         0\n",
       "monthoct         0\n",
       "monthsep         0\n",
       "size_category    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forestfires.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0f650270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FFMC</th>\n",
       "      <th>DMC</th>\n",
       "      <th>DC</th>\n",
       "      <th>ISI</th>\n",
       "      <th>temp</th>\n",
       "      <th>RH</th>\n",
       "      <th>wind</th>\n",
       "      <th>rain</th>\n",
       "      <th>area</th>\n",
       "      <th>dayfri</th>\n",
       "      <th>...</th>\n",
       "      <th>monthdec</th>\n",
       "      <th>monthfeb</th>\n",
       "      <th>monthjan</th>\n",
       "      <th>monthjul</th>\n",
       "      <th>monthjun</th>\n",
       "      <th>monthmar</th>\n",
       "      <th>monthmay</th>\n",
       "      <th>monthnov</th>\n",
       "      <th>monthoct</th>\n",
       "      <th>monthsep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "      <td>517.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>90.644681</td>\n",
       "      <td>110.872340</td>\n",
       "      <td>547.940039</td>\n",
       "      <td>9.021663</td>\n",
       "      <td>18.889168</td>\n",
       "      <td>44.288201</td>\n",
       "      <td>4.017602</td>\n",
       "      <td>0.021663</td>\n",
       "      <td>12.847292</td>\n",
       "      <td>0.164410</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017408</td>\n",
       "      <td>0.038685</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.061896</td>\n",
       "      <td>0.032882</td>\n",
       "      <td>0.104449</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.001934</td>\n",
       "      <td>0.029014</td>\n",
       "      <td>0.332689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.520111</td>\n",
       "      <td>64.046482</td>\n",
       "      <td>248.066192</td>\n",
       "      <td>4.559477</td>\n",
       "      <td>5.806625</td>\n",
       "      <td>16.317469</td>\n",
       "      <td>1.791653</td>\n",
       "      <td>0.295959</td>\n",
       "      <td>63.655818</td>\n",
       "      <td>0.371006</td>\n",
       "      <td>...</td>\n",
       "      <td>0.130913</td>\n",
       "      <td>0.193029</td>\n",
       "      <td>0.062137</td>\n",
       "      <td>0.241199</td>\n",
       "      <td>0.178500</td>\n",
       "      <td>0.306138</td>\n",
       "      <td>0.062137</td>\n",
       "      <td>0.043980</td>\n",
       "      <td>0.168007</td>\n",
       "      <td>0.471632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>18.700000</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>90.200000</td>\n",
       "      <td>68.600000</td>\n",
       "      <td>437.700000</td>\n",
       "      <td>6.500000</td>\n",
       "      <td>15.500000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>2.700000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>91.600000</td>\n",
       "      <td>108.300000</td>\n",
       "      <td>664.200000</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>19.300000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>92.900000</td>\n",
       "      <td>142.400000</td>\n",
       "      <td>713.900000</td>\n",
       "      <td>10.800000</td>\n",
       "      <td>22.800000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.570000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>96.200000</td>\n",
       "      <td>291.300000</td>\n",
       "      <td>860.600000</td>\n",
       "      <td>56.100000</td>\n",
       "      <td>33.300000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>9.400000</td>\n",
       "      <td>6.400000</td>\n",
       "      <td>1090.840000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             FFMC         DMC          DC         ISI        temp          RH  \\\n",
       "count  517.000000  517.000000  517.000000  517.000000  517.000000  517.000000   \n",
       "mean    90.644681  110.872340  547.940039    9.021663   18.889168   44.288201   \n",
       "std      5.520111   64.046482  248.066192    4.559477    5.806625   16.317469   \n",
       "min     18.700000    1.100000    7.900000    0.000000    2.200000   15.000000   \n",
       "25%     90.200000   68.600000  437.700000    6.500000   15.500000   33.000000   \n",
       "50%     91.600000  108.300000  664.200000    8.400000   19.300000   42.000000   \n",
       "75%     92.900000  142.400000  713.900000   10.800000   22.800000   53.000000   \n",
       "max     96.200000  291.300000  860.600000   56.100000   33.300000  100.000000   \n",
       "\n",
       "             wind        rain         area      dayfri  ...    monthdec  \\\n",
       "count  517.000000  517.000000   517.000000  517.000000  ...  517.000000   \n",
       "mean     4.017602    0.021663    12.847292    0.164410  ...    0.017408   \n",
       "std      1.791653    0.295959    63.655818    0.371006  ...    0.130913   \n",
       "min      0.400000    0.000000     0.000000    0.000000  ...    0.000000   \n",
       "25%      2.700000    0.000000     0.000000    0.000000  ...    0.000000   \n",
       "50%      4.000000    0.000000     0.520000    0.000000  ...    0.000000   \n",
       "75%      4.900000    0.000000     6.570000    0.000000  ...    0.000000   \n",
       "max      9.400000    6.400000  1090.840000    1.000000  ...    1.000000   \n",
       "\n",
       "         monthfeb    monthjan    monthjul    monthjun    monthmar    monthmay  \\\n",
       "count  517.000000  517.000000  517.000000  517.000000  517.000000  517.000000   \n",
       "mean     0.038685    0.003868    0.061896    0.032882    0.104449    0.003868   \n",
       "std      0.193029    0.062137    0.241199    0.178500    0.306138    0.062137   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "75%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "         monthnov    monthoct    monthsep  \n",
       "count  517.000000  517.000000  517.000000  \n",
       "mean     0.001934    0.029014    0.332689  \n",
       "std      0.043980    0.168007    0.471632  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      0.000000    0.000000    0.000000  \n",
       "50%      0.000000    0.000000    0.000000  \n",
       "75%      0.000000    0.000000    1.000000  \n",
       "max      1.000000    1.000000    1.000000  \n",
       "\n",
       "[8 rows x 28 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forestfires.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb91a3a",
   "metadata": {},
   "source": [
    "# taking small as 0 and large as 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5f8925a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    378\n",
       "1    139\n",
       "Name: size_category, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forestfires.loc[forestfires[\"size_category\"]=='small','size_category']=0\n",
    "forestfires.loc[forestfires[\"size_category\"]=='large','size_category']=1\n",
    "forestfires[\"size_category\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c0852c",
   "metadata": {},
   "source": [
    "# normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f45f5c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_func(i):\n",
    "     x = (i-i.min())/(i.max()-i.min())\n",
    "     return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6899a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = forestfires.iloc[:,0:28]\n",
    "target = forestfires.iloc[:,28]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b3e78cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors1 = norm_func(predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "40516a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test= train_test_split(predictors1,target, test_size=0.3,stratify = target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6565f007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_model(hidden_dim):\n",
    "    model = Sequential()\n",
    "    for i in range(1,len(hidden_dim)-1):\n",
    "        if (i==1):\n",
    "            model.add(Dense(hidden_dim[i],input_dim=hidden_dim[0],activation=\"relu\"))\n",
    "        else:\n",
    "            model.add(Dense(hidden_dim[i],activation=\"relu\"))\n",
    "    model.add(Dense(hidden_dim[-1],kernel_initializer=\"normal\",activation=\"sigmoid\"))\n",
    "    model.compile(loss=\"binary_crossentropy\",\n",
    "                  optimizer = \"rmsprop\",\n",
    "                  metrics = [\"accuracy\"])\n",
    "    return model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ef689449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TRUPTI~1\\AppData\\Local\\Temp/ipykernel_25968/2943197496.py:2: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  first_model.fit(np.array(x_train).astype(np.int),np.array(y_train).astype(np.int),epochs=500)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 1s 3ms/step - loss: 0.6774 - accuracy: 0.6981\n",
      "Epoch 2/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.6441 - accuracy: 0.7313\n",
      "Epoch 3/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.6121 - accuracy: 0.7313\n",
      "Epoch 4/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5910 - accuracy: 0.7313\n",
      "Epoch 5/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5830 - accuracy: 0.7313\n",
      "Epoch 6/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5779 - accuracy: 0.7313\n",
      "Epoch 7/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5747 - accuracy: 0.7313\n",
      "Epoch 8/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5732 - accuracy: 0.7313\n",
      "Epoch 9/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5699 - accuracy: 0.7313\n",
      "Epoch 10/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5690 - accuracy: 0.7313\n",
      "Epoch 11/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5679 - accuracy: 0.7313\n",
      "Epoch 12/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5658 - accuracy: 0.7313\n",
      "Epoch 13/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5650 - accuracy: 0.7313\n",
      "Epoch 14/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5633 - accuracy: 0.7313\n",
      "Epoch 15/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5618 - accuracy: 0.7313\n",
      "Epoch 16/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5613 - accuracy: 0.7313\n",
      "Epoch 17/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5596 - accuracy: 0.7313\n",
      "Epoch 18/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5579 - accuracy: 0.7313\n",
      "Epoch 19/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5571 - accuracy: 0.7313\n",
      "Epoch 20/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5563 - accuracy: 0.7313\n",
      "Epoch 21/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5548 - accuracy: 0.7313\n",
      "Epoch 22/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5536 - accuracy: 0.7313\n",
      "Epoch 23/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5530 - accuracy: 0.7313\n",
      "Epoch 24/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5506 - accuracy: 0.7313\n",
      "Epoch 25/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5500 - accuracy: 0.7313\n",
      "Epoch 26/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5481 - accuracy: 0.7313\n",
      "Epoch 27/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5478 - accuracy: 0.7313\n",
      "Epoch 28/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5461 - accuracy: 0.7313\n",
      "Epoch 29/500\n",
      "12/12 [==============================] - 0s 5ms/step - loss: 0.5449 - accuracy: 0.7313\n",
      "Epoch 30/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5440 - accuracy: 0.7341\n",
      "Epoch 31/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5424 - accuracy: 0.7341\n",
      "Epoch 32/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5426 - accuracy: 0.7368\n",
      "Epoch 33/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5409 - accuracy: 0.7368\n",
      "Epoch 34/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5387 - accuracy: 0.7368\n",
      "Epoch 35/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5380 - accuracy: 0.7368\n",
      "Epoch 36/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5371 - accuracy: 0.7396\n",
      "Epoch 37/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5356 - accuracy: 0.7396\n",
      "Epoch 38/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5349 - accuracy: 0.7452\n",
      "Epoch 39/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5332 - accuracy: 0.7535\n",
      "Epoch 40/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5325 - accuracy: 0.7507\n",
      "Epoch 41/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5312 - accuracy: 0.7562\n",
      "Epoch 42/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5292 - accuracy: 0.7562\n",
      "Epoch 43/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5285 - accuracy: 0.7535\n",
      "Epoch 44/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5276 - accuracy: 0.7535\n",
      "Epoch 45/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5272 - accuracy: 0.7618\n",
      "Epoch 46/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5259 - accuracy: 0.7562\n",
      "Epoch 47/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5235 - accuracy: 0.7562\n",
      "Epoch 48/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5224 - accuracy: 0.7562\n",
      "Epoch 49/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5211 - accuracy: 0.7618\n",
      "Epoch 50/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5189 - accuracy: 0.7590\n",
      "Epoch 51/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5196 - accuracy: 0.7618\n",
      "Epoch 52/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5185 - accuracy: 0.7618\n",
      "Epoch 53/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5170 - accuracy: 0.7618\n",
      "Epoch 54/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5155 - accuracy: 0.7590\n",
      "Epoch 55/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5151 - accuracy: 0.7618\n",
      "Epoch 56/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5134 - accuracy: 0.7618\n",
      "Epoch 57/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5134 - accuracy: 0.7618\n",
      "Epoch 58/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5124 - accuracy: 0.7590\n",
      "Epoch 59/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5109 - accuracy: 0.7618\n",
      "Epoch 60/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.5111 - accuracy: 0.7618\n",
      "Epoch 61/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5110 - accuracy: 0.7618\n",
      "Epoch 62/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5091 - accuracy: 0.7618\n",
      "Epoch 63/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5072 - accuracy: 0.7618\n",
      "Epoch 64/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5073 - accuracy: 0.7618\n",
      "Epoch 65/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5062 - accuracy: 0.7645\n",
      "Epoch 66/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5070 - accuracy: 0.7618\n",
      "Epoch 67/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5037 - accuracy: 0.7590\n",
      "Epoch 68/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5042 - accuracy: 0.7645\n",
      "Epoch 69/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.5034 - accuracy: 0.7618\n",
      "Epoch 70/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5036 - accuracy: 0.7562\n",
      "Epoch 71/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5037 - accuracy: 0.7645\n",
      "Epoch 72/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5021 - accuracy: 0.7645\n",
      "Epoch 73/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5008 - accuracy: 0.7590\n",
      "Epoch 74/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5009 - accuracy: 0.7645\n",
      "Epoch 75/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5015 - accuracy: 0.7562\n",
      "Epoch 76/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.5004 - accuracy: 0.7535\n",
      "Epoch 77/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4987 - accuracy: 0.7618\n",
      "Epoch 78/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4998 - accuracy: 0.7618\n",
      "Epoch 79/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4988 - accuracy: 0.7645\n",
      "Epoch 80/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4977 - accuracy: 0.7590\n",
      "Epoch 81/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4975 - accuracy: 0.7590\n",
      "Epoch 82/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4966 - accuracy: 0.7645\n",
      "Epoch 83/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4966 - accuracy: 0.7645\n",
      "Epoch 84/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4986 - accuracy: 0.7618\n",
      "Epoch 85/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4965 - accuracy: 0.7645\n",
      "Epoch 86/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4964 - accuracy: 0.7562\n",
      "Epoch 87/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4955 - accuracy: 0.7618\n",
      "Epoch 88/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4955 - accuracy: 0.7590\n",
      "Epoch 89/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4941 - accuracy: 0.7645\n",
      "Epoch 90/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4938 - accuracy: 0.7645\n",
      "Epoch 91/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4965 - accuracy: 0.7673\n",
      "Epoch 92/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4941 - accuracy: 0.7673\n",
      "Epoch 93/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4938 - accuracy: 0.7673\n",
      "Epoch 94/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4927 - accuracy: 0.7618\n",
      "Epoch 95/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4941 - accuracy: 0.7673\n",
      "Epoch 96/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4942 - accuracy: 0.7673\n",
      "Epoch 97/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4946 - accuracy: 0.7645\n",
      "Epoch 98/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4919 - accuracy: 0.7590\n",
      "Epoch 99/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4926 - accuracy: 0.7562\n",
      "Epoch 100/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4953 - accuracy: 0.7590\n",
      "Epoch 101/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4912 - accuracy: 0.7673\n",
      "Epoch 102/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4913 - accuracy: 0.7645\n",
      "Epoch 103/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4923 - accuracy: 0.7590\n",
      "Epoch 104/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4948 - accuracy: 0.7618\n",
      "Epoch 105/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4929 - accuracy: 0.7673\n",
      "Epoch 106/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4910 - accuracy: 0.7590\n",
      "Epoch 107/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4902 - accuracy: 0.7645\n",
      "Epoch 108/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4923 - accuracy: 0.7618\n",
      "Epoch 109/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4903 - accuracy: 0.7645\n",
      "Epoch 110/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4892 - accuracy: 0.7645\n",
      "Epoch 111/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4896 - accuracy: 0.7645\n",
      "Epoch 112/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4910 - accuracy: 0.7507\n",
      "Epoch 113/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4873 - accuracy: 0.7673\n",
      "Epoch 114/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4903 - accuracy: 0.7673\n",
      "Epoch 115/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4901 - accuracy: 0.7645\n",
      "Epoch 116/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4889 - accuracy: 0.7535\n",
      "Epoch 117/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4913 - accuracy: 0.7562\n",
      "Epoch 118/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4879 - accuracy: 0.7590\n",
      "Epoch 119/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4883 - accuracy: 0.7673\n",
      "Epoch 120/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4902 - accuracy: 0.7590\n",
      "Epoch 121/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4872 - accuracy: 0.7562\n",
      "Epoch 122/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4873 - accuracy: 0.7618\n",
      "Epoch 123/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4881 - accuracy: 0.7673\n",
      "Epoch 124/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4880 - accuracy: 0.7590\n",
      "Epoch 125/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4882 - accuracy: 0.7673\n",
      "Epoch 126/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4878 - accuracy: 0.7618\n",
      "Epoch 127/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4902 - accuracy: 0.7701\n",
      "Epoch 128/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4864 - accuracy: 0.7590\n",
      "Epoch 129/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4887 - accuracy: 0.7618\n",
      "Epoch 130/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4887 - accuracy: 0.7645\n",
      "Epoch 131/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4887 - accuracy: 0.7618\n",
      "Epoch 132/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4867 - accuracy: 0.7618\n",
      "Epoch 133/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4877 - accuracy: 0.7618\n",
      "Epoch 134/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4881 - accuracy: 0.7590\n",
      "Epoch 135/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4894 - accuracy: 0.7645\n",
      "Epoch 136/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4876 - accuracy: 0.7618\n",
      "Epoch 137/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4869 - accuracy: 0.7645\n",
      "Epoch 138/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4866 - accuracy: 0.7645\n",
      "Epoch 139/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4869 - accuracy: 0.7535\n",
      "Epoch 140/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4860 - accuracy: 0.7535\n",
      "Epoch 141/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4855 - accuracy: 0.7562\n",
      "Epoch 142/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4865 - accuracy: 0.7673\n",
      "Epoch 143/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4853 - accuracy: 0.7645\n",
      "Epoch 144/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4871 - accuracy: 0.7562\n",
      "Epoch 145/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4876 - accuracy: 0.7701\n",
      "Epoch 146/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4856 - accuracy: 0.7562\n",
      "Epoch 147/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4852 - accuracy: 0.7618\n",
      "Epoch 148/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4866 - accuracy: 0.7784\n",
      "Epoch 149/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4862 - accuracy: 0.7701\n",
      "Epoch 150/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4865 - accuracy: 0.7645\n",
      "Epoch 151/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4868 - accuracy: 0.7562\n",
      "Epoch 152/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4849 - accuracy: 0.7645\n",
      "Epoch 153/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4865 - accuracy: 0.7673\n",
      "Epoch 154/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4856 - accuracy: 0.7562\n",
      "Epoch 155/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4868 - accuracy: 0.7673\n",
      "Epoch 156/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4858 - accuracy: 0.7590\n",
      "Epoch 157/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4853 - accuracy: 0.7645\n",
      "Epoch 158/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4848 - accuracy: 0.7590\n",
      "Epoch 159/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4869 - accuracy: 0.7701\n",
      "Epoch 160/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4854 - accuracy: 0.7618\n",
      "Epoch 161/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4848 - accuracy: 0.7673\n",
      "Epoch 162/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4843 - accuracy: 0.7701\n",
      "Epoch 163/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4844 - accuracy: 0.7590\n",
      "Epoch 164/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4866 - accuracy: 0.7590\n",
      "Epoch 165/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4844 - accuracy: 0.7618\n",
      "Epoch 166/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4843 - accuracy: 0.7645\n",
      "Epoch 167/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4859 - accuracy: 0.7618\n",
      "Epoch 168/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4850 - accuracy: 0.7673\n",
      "Epoch 169/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4838 - accuracy: 0.7673\n",
      "Epoch 170/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4855 - accuracy: 0.7590\n",
      "Epoch 171/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4838 - accuracy: 0.7590\n",
      "Epoch 172/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4857 - accuracy: 0.7645\n",
      "Epoch 173/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4832 - accuracy: 0.7590\n",
      "Epoch 174/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4850 - accuracy: 0.7590\n",
      "Epoch 175/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4874 - accuracy: 0.7562\n",
      "Epoch 176/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4843 - accuracy: 0.7562\n",
      "Epoch 177/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4840 - accuracy: 0.7645\n",
      "Epoch 178/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4845 - accuracy: 0.7673\n",
      "Epoch 179/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4852 - accuracy: 0.7701\n",
      "Epoch 180/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4836 - accuracy: 0.7618\n",
      "Epoch 181/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4873 - accuracy: 0.7618\n",
      "Epoch 182/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4848 - accuracy: 0.7535\n",
      "Epoch 183/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4840 - accuracy: 0.7645\n",
      "Epoch 184/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4829 - accuracy: 0.7590\n",
      "Epoch 185/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4866 - accuracy: 0.7645\n",
      "Epoch 186/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4833 - accuracy: 0.7590\n",
      "Epoch 187/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4843 - accuracy: 0.7590\n",
      "Epoch 188/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4829 - accuracy: 0.7562\n",
      "Epoch 189/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4840 - accuracy: 0.7618\n",
      "Epoch 190/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4830 - accuracy: 0.7618\n",
      "Epoch 191/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4844 - accuracy: 0.7673\n",
      "Epoch 192/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4834 - accuracy: 0.7701\n",
      "Epoch 193/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.7618\n",
      "Epoch 194/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4824 - accuracy: 0.7701\n",
      "Epoch 195/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4831 - accuracy: 0.7618\n",
      "Epoch 196/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4842 - accuracy: 0.7673\n",
      "Epoch 197/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4842 - accuracy: 0.7562\n",
      "Epoch 198/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4833 - accuracy: 0.7645\n",
      "Epoch 199/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4837 - accuracy: 0.7645\n",
      "Epoch 200/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4825 - accuracy: 0.7645\n",
      "Epoch 201/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4845 - accuracy: 0.7645\n",
      "Epoch 202/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4832 - accuracy: 0.7618\n",
      "Epoch 203/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4825 - accuracy: 0.7701\n",
      "Epoch 204/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4817 - accuracy: 0.7673\n",
      "Epoch 205/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4843 - accuracy: 0.7618\n",
      "Epoch 206/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4840 - accuracy: 0.7645\n",
      "Epoch 207/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4815 - accuracy: 0.7590\n",
      "Epoch 208/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4853 - accuracy: 0.7673\n",
      "Epoch 209/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4844 - accuracy: 0.7645\n",
      "Epoch 210/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4815 - accuracy: 0.7645\n",
      "Epoch 211/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4835 - accuracy: 0.7618\n",
      "Epoch 212/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4832 - accuracy: 0.7590\n",
      "Epoch 213/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4841 - accuracy: 0.7618\n",
      "Epoch 214/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4828 - accuracy: 0.7618\n",
      "Epoch 215/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4825 - accuracy: 0.7618\n",
      "Epoch 216/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4828 - accuracy: 0.7590\n",
      "Epoch 217/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4845 - accuracy: 0.7562\n",
      "Epoch 218/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4823 - accuracy: 0.7645\n",
      "Epoch 219/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4832 - accuracy: 0.7673\n",
      "Epoch 220/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4811 - accuracy: 0.7618\n",
      "Epoch 221/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4829 - accuracy: 0.7645\n",
      "Epoch 222/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4843 - accuracy: 0.7673\n",
      "Epoch 223/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4824 - accuracy: 0.7535\n",
      "Epoch 224/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4835 - accuracy: 0.7645\n",
      "Epoch 225/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4844 - accuracy: 0.7645\n",
      "Epoch 226/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4833 - accuracy: 0.7535\n",
      "Epoch 227/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4849 - accuracy: 0.7479\n",
      "Epoch 228/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4819 - accuracy: 0.7618\n",
      "Epoch 229/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4840 - accuracy: 0.7701\n",
      "Epoch 230/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.7618\n",
      "Epoch 231/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4828 - accuracy: 0.7618\n",
      "Epoch 232/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4828 - accuracy: 0.7673\n",
      "Epoch 233/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4793 - accuracy: 0.7590\n",
      "Epoch 234/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4839 - accuracy: 0.7673\n",
      "Epoch 235/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4827 - accuracy: 0.7645\n",
      "Epoch 236/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4848 - accuracy: 0.7618\n",
      "Epoch 237/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4817 - accuracy: 0.7618\n",
      "Epoch 238/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4821 - accuracy: 0.7673\n",
      "Epoch 239/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4819 - accuracy: 0.7645\n",
      "Epoch 240/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4819 - accuracy: 0.7645\n",
      "Epoch 241/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4823 - accuracy: 0.7618\n",
      "Epoch 242/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4829 - accuracy: 0.7645\n",
      "Epoch 243/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4846 - accuracy: 0.7590\n",
      "Epoch 244/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4815 - accuracy: 0.7618\n",
      "Epoch 245/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4825 - accuracy: 0.7673\n",
      "Epoch 246/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4829 - accuracy: 0.7535\n",
      "Epoch 247/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4839 - accuracy: 0.7618\n",
      "Epoch 248/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4805 - accuracy: 0.7645\n",
      "Epoch 249/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4828 - accuracy: 0.7645\n",
      "Epoch 250/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4833 - accuracy: 0.7618\n",
      "Epoch 251/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4828 - accuracy: 0.7645\n",
      "Epoch 252/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4828 - accuracy: 0.7673\n",
      "Epoch 253/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4818 - accuracy: 0.7673\n",
      "Epoch 254/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4820 - accuracy: 0.7645\n",
      "Epoch 255/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4849 - accuracy: 0.7673\n",
      "Epoch 256/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4808 - accuracy: 0.7645\n",
      "Epoch 257/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4820 - accuracy: 0.7645\n",
      "Epoch 258/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4832 - accuracy: 0.7562\n",
      "Epoch 259/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4825 - accuracy: 0.7645\n",
      "Epoch 260/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4809 - accuracy: 0.7618\n",
      "Epoch 261/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4826 - accuracy: 0.7590\n",
      "Epoch 262/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4812 - accuracy: 0.7673\n",
      "Epoch 263/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4828 - accuracy: 0.7562\n",
      "Epoch 264/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4831 - accuracy: 0.7673\n",
      "Epoch 265/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4811 - accuracy: 0.7590\n",
      "Epoch 266/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4822 - accuracy: 0.7645\n",
      "Epoch 267/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4813 - accuracy: 0.7645\n",
      "Epoch 268/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4834 - accuracy: 0.7562\n",
      "Epoch 269/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4820 - accuracy: 0.7673\n",
      "Epoch 270/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4822 - accuracy: 0.7590\n",
      "Epoch 271/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4815 - accuracy: 0.7618\n",
      "Epoch 272/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4827 - accuracy: 0.7618\n",
      "Epoch 273/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4812 - accuracy: 0.7645\n",
      "Epoch 274/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4819 - accuracy: 0.7645\n",
      "Epoch 275/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4812 - accuracy: 0.7673\n",
      "Epoch 276/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4812 - accuracy: 0.7673\n",
      "Epoch 277/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4815 - accuracy: 0.7673\n",
      "Epoch 278/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4823 - accuracy: 0.7673\n",
      "Epoch 279/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4821 - accuracy: 0.7673\n",
      "Epoch 280/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4808 - accuracy: 0.7618\n",
      "Epoch 281/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4820 - accuracy: 0.7618\n",
      "Epoch 282/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4818 - accuracy: 0.7590\n",
      "Epoch 283/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4823 - accuracy: 0.7562\n",
      "Epoch 284/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4815 - accuracy: 0.7535\n",
      "Epoch 285/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4810 - accuracy: 0.7645\n",
      "Epoch 286/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4812 - accuracy: 0.7673\n",
      "Epoch 287/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4828 - accuracy: 0.7618\n",
      "Epoch 288/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4823 - accuracy: 0.7645\n",
      "Epoch 289/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4814 - accuracy: 0.7645\n",
      "Epoch 290/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4817 - accuracy: 0.7701\n",
      "Epoch 291/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4824 - accuracy: 0.7562\n",
      "Epoch 292/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4829 - accuracy: 0.7590\n",
      "Epoch 293/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4807 - accuracy: 0.7618\n",
      "Epoch 294/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4831 - accuracy: 0.7673\n",
      "Epoch 295/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4810 - accuracy: 0.7645\n",
      "Epoch 296/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4819 - accuracy: 0.7645\n",
      "Epoch 297/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4810 - accuracy: 0.7618\n",
      "Epoch 298/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4802 - accuracy: 0.7645\n",
      "Epoch 299/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4830 - accuracy: 0.7590\n",
      "Epoch 300/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4823 - accuracy: 0.7618\n",
      "Epoch 301/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4820 - accuracy: 0.7562\n",
      "Epoch 302/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4830 - accuracy: 0.7645\n",
      "Epoch 303/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4812 - accuracy: 0.7590\n",
      "Epoch 304/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4811 - accuracy: 0.7645\n",
      "Epoch 305/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4805 - accuracy: 0.7618\n",
      "Epoch 306/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4827 - accuracy: 0.7590\n",
      "Epoch 307/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4811 - accuracy: 0.7618\n",
      "Epoch 308/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4825 - accuracy: 0.7618\n",
      "Epoch 309/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4814 - accuracy: 0.7645\n",
      "Epoch 310/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4810 - accuracy: 0.7618\n",
      "Epoch 311/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4809 - accuracy: 0.7645\n",
      "Epoch 312/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4823 - accuracy: 0.7479\n",
      "Epoch 313/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4829 - accuracy: 0.7562\n",
      "Epoch 314/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4814 - accuracy: 0.7673\n",
      "Epoch 315/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4805 - accuracy: 0.7618\n",
      "Epoch 316/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4814 - accuracy: 0.7645\n",
      "Epoch 317/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4810 - accuracy: 0.7590\n",
      "Epoch 318/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4817 - accuracy: 0.7562\n",
      "Epoch 319/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4816 - accuracy: 0.7618\n",
      "Epoch 320/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4803 - accuracy: 0.7618\n",
      "Epoch 321/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4807 - accuracy: 0.7673\n",
      "Epoch 322/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4808 - accuracy: 0.7645\n",
      "Epoch 323/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4825 - accuracy: 0.7645\n",
      "Epoch 324/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4804 - accuracy: 0.7590\n",
      "Epoch 325/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4808 - accuracy: 0.7645\n",
      "Epoch 326/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4813 - accuracy: 0.7590\n",
      "Epoch 327/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4828 - accuracy: 0.7645\n",
      "Epoch 328/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4814 - accuracy: 0.7645\n",
      "Epoch 329/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4810 - accuracy: 0.7507\n",
      "Epoch 330/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4807 - accuracy: 0.7673\n",
      "Epoch 331/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4809 - accuracy: 0.7673\n",
      "Epoch 332/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4816 - accuracy: 0.7562\n",
      "Epoch 333/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4810 - accuracy: 0.7645\n",
      "Epoch 334/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4826 - accuracy: 0.7645\n",
      "Epoch 335/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4812 - accuracy: 0.7673\n",
      "Epoch 336/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4814 - accuracy: 0.7618\n",
      "Epoch 337/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4799 - accuracy: 0.7618\n",
      "Epoch 338/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4801 - accuracy: 0.7701\n",
      "Epoch 339/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4807 - accuracy: 0.7673\n",
      "Epoch 340/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4819 - accuracy: 0.7618\n",
      "Epoch 341/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4806 - accuracy: 0.7673\n",
      "Epoch 342/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4809 - accuracy: 0.7535\n",
      "Epoch 343/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4813 - accuracy: 0.7645\n",
      "Epoch 344/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4808 - accuracy: 0.7673\n",
      "Epoch 345/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4806 - accuracy: 0.7618\n",
      "Epoch 346/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4810 - accuracy: 0.7618\n",
      "Epoch 347/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4798 - accuracy: 0.7618\n",
      "Epoch 348/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4814 - accuracy: 0.7645\n",
      "Epoch 349/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4796 - accuracy: 0.7673\n",
      "Epoch 350/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4820 - accuracy: 0.7645\n",
      "Epoch 351/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4795 - accuracy: 0.7673\n",
      "Epoch 352/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4804 - accuracy: 0.7673\n",
      "Epoch 353/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4801 - accuracy: 0.7645\n",
      "Epoch 354/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4809 - accuracy: 0.7562\n",
      "Epoch 355/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4824 - accuracy: 0.7618\n",
      "Epoch 356/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4794 - accuracy: 0.7618\n",
      "Epoch 357/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4795 - accuracy: 0.7618\n",
      "Epoch 358/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4811 - accuracy: 0.7618\n",
      "Epoch 359/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4814 - accuracy: 0.7618\n",
      "Epoch 360/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4809 - accuracy: 0.7645\n",
      "Epoch 361/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4821 - accuracy: 0.7645\n",
      "Epoch 362/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4812 - accuracy: 0.7507\n",
      "Epoch 363/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4808 - accuracy: 0.7479\n",
      "Epoch 364/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4804 - accuracy: 0.7618\n",
      "Epoch 365/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4819 - accuracy: 0.7590\n",
      "Epoch 366/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4805 - accuracy: 0.7645\n",
      "Epoch 367/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4812 - accuracy: 0.7590\n",
      "Epoch 368/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4790 - accuracy: 0.7645\n",
      "Epoch 369/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4809 - accuracy: 0.7645\n",
      "Epoch 370/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4814 - accuracy: 0.7645\n",
      "Epoch 371/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4811 - accuracy: 0.7673\n",
      "Epoch 372/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4809 - accuracy: 0.7618\n",
      "Epoch 373/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4794 - accuracy: 0.7535\n",
      "Epoch 374/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4796 - accuracy: 0.7618\n",
      "Epoch 375/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4828 - accuracy: 0.7645\n",
      "Epoch 376/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4800 - accuracy: 0.7562\n",
      "Epoch 377/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4798 - accuracy: 0.7618\n",
      "Epoch 378/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4803 - accuracy: 0.7645\n",
      "Epoch 379/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4839 - accuracy: 0.7590\n",
      "Epoch 380/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4791 - accuracy: 0.7645\n",
      "Epoch 381/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4804 - accuracy: 0.7590\n",
      "Epoch 382/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4803 - accuracy: 0.7618\n",
      "Epoch 383/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4806 - accuracy: 0.7590\n",
      "Epoch 384/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4813 - accuracy: 0.7535\n",
      "Epoch 385/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4795 - accuracy: 0.7618\n",
      "Epoch 386/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4806 - accuracy: 0.7645\n",
      "Epoch 387/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4797 - accuracy: 0.7673\n",
      "Epoch 388/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4788 - accuracy: 0.7645\n",
      "Epoch 389/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4806 - accuracy: 0.7645\n",
      "Epoch 390/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4798 - accuracy: 0.7590\n",
      "Epoch 391/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4803 - accuracy: 0.7645\n",
      "Epoch 392/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4814 - accuracy: 0.7618\n",
      "Epoch 393/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4804 - accuracy: 0.7673\n",
      "Epoch 394/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4803 - accuracy: 0.7645\n",
      "Epoch 395/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4800 - accuracy: 0.7645\n",
      "Epoch 396/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4805 - accuracy: 0.7535\n",
      "Epoch 397/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4799 - accuracy: 0.7535\n",
      "Epoch 398/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4792 - accuracy: 0.7618\n",
      "Epoch 399/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4799 - accuracy: 0.7562\n",
      "Epoch 400/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4812 - accuracy: 0.7645\n",
      "Epoch 401/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4816 - accuracy: 0.7618\n",
      "Epoch 402/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4801 - accuracy: 0.7645\n",
      "Epoch 403/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4807 - accuracy: 0.7701\n",
      "Epoch 404/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4801 - accuracy: 0.7618\n",
      "Epoch 405/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4793 - accuracy: 0.7590\n",
      "Epoch 406/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4809 - accuracy: 0.7673\n",
      "Epoch 407/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4804 - accuracy: 0.7645\n",
      "Epoch 408/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4801 - accuracy: 0.7645\n",
      "Epoch 409/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4797 - accuracy: 0.7645\n",
      "Epoch 410/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4796 - accuracy: 0.7618\n",
      "Epoch 411/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4801 - accuracy: 0.7645\n",
      "Epoch 412/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4818 - accuracy: 0.7645\n",
      "Epoch 413/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4791 - accuracy: 0.7645\n",
      "Epoch 414/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4788 - accuracy: 0.7618\n",
      "Epoch 415/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4796 - accuracy: 0.7590\n",
      "Epoch 416/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4799 - accuracy: 0.7645\n",
      "Epoch 417/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4807 - accuracy: 0.7535\n",
      "Epoch 418/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4797 - accuracy: 0.7562\n",
      "Epoch 419/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4810 - accuracy: 0.7562\n",
      "Epoch 420/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4800 - accuracy: 0.7645\n",
      "Epoch 421/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4809 - accuracy: 0.7618\n",
      "Epoch 422/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4799 - accuracy: 0.7535\n",
      "Epoch 423/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4810 - accuracy: 0.7590\n",
      "Epoch 424/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4794 - accuracy: 0.7645\n",
      "Epoch 425/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4816 - accuracy: 0.7535\n",
      "Epoch 426/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4803 - accuracy: 0.7645\n",
      "Epoch 427/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4797 - accuracy: 0.7673\n",
      "Epoch 428/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4791 - accuracy: 0.7645\n",
      "Epoch 429/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4796 - accuracy: 0.7645\n",
      "Epoch 430/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4784 - accuracy: 0.7645\n",
      "Epoch 431/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4807 - accuracy: 0.7618\n",
      "Epoch 432/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4809 - accuracy: 0.7645\n",
      "Epoch 433/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4793 - accuracy: 0.7590\n",
      "Epoch 434/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4784 - accuracy: 0.7645\n",
      "Epoch 435/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4797 - accuracy: 0.7645\n",
      "Epoch 436/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4795 - accuracy: 0.7590\n",
      "Epoch 437/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4797 - accuracy: 0.7645\n",
      "Epoch 438/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4800 - accuracy: 0.7673\n",
      "Epoch 439/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4795 - accuracy: 0.7673\n",
      "Epoch 440/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4787 - accuracy: 0.7618\n",
      "Epoch 441/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4795 - accuracy: 0.7618\n",
      "Epoch 442/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4809 - accuracy: 0.7673\n",
      "Epoch 443/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4807 - accuracy: 0.7673\n",
      "Epoch 444/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4803 - accuracy: 0.7618\n",
      "Epoch 445/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4781 - accuracy: 0.7618\n",
      "Epoch 446/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4796 - accuracy: 0.7590\n",
      "Epoch 447/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4801 - accuracy: 0.7562\n",
      "Epoch 448/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4796 - accuracy: 0.7618\n",
      "Epoch 449/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4792 - accuracy: 0.7645\n",
      "Epoch 450/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4795 - accuracy: 0.7618\n",
      "Epoch 451/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4793 - accuracy: 0.7673\n",
      "Epoch 452/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4793 - accuracy: 0.7590\n",
      "Epoch 453/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4790 - accuracy: 0.7618\n",
      "Epoch 454/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4798 - accuracy: 0.7618\n",
      "Epoch 455/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4791 - accuracy: 0.7590\n",
      "Epoch 456/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4792 - accuracy: 0.7535\n",
      "Epoch 457/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4795 - accuracy: 0.7645\n",
      "Epoch 458/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4790 - accuracy: 0.7618\n",
      "Epoch 459/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4803 - accuracy: 0.7618\n",
      "Epoch 460/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4803 - accuracy: 0.7535\n",
      "Epoch 461/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4780 - accuracy: 0.7673\n",
      "Epoch 462/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4800 - accuracy: 0.7562\n",
      "Epoch 463/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4788 - accuracy: 0.7590\n",
      "Epoch 464/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4804 - accuracy: 0.7673\n",
      "Epoch 465/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4790 - accuracy: 0.7590\n",
      "Epoch 466/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4789 - accuracy: 0.7645\n",
      "Epoch 467/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4792 - accuracy: 0.7645\n",
      "Epoch 468/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4789 - accuracy: 0.7645\n",
      "Epoch 469/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4801 - accuracy: 0.7618\n",
      "Epoch 470/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4810 - accuracy: 0.7562\n",
      "Epoch 471/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4799 - accuracy: 0.7673\n",
      "Epoch 472/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4796 - accuracy: 0.7673\n",
      "Epoch 473/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4798 - accuracy: 0.7645\n",
      "Epoch 474/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4788 - accuracy: 0.7562\n",
      "Epoch 475/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4791 - accuracy: 0.7701\n",
      "Epoch 476/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4803 - accuracy: 0.7645\n",
      "Epoch 477/500\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.4797 - accuracy: 0.7701\n",
      "Epoch 478/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4785 - accuracy: 0.7645\n",
      "Epoch 479/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4789 - accuracy: 0.7590\n",
      "Epoch 480/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4797 - accuracy: 0.7590\n",
      "Epoch 481/500\n",
      "12/12 [==============================] - 0s 4ms/step - loss: 0.4789 - accuracy: 0.7618\n",
      "Epoch 482/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4796 - accuracy: 0.7645\n",
      "Epoch 483/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4805 - accuracy: 0.7590\n",
      "Epoch 484/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4793 - accuracy: 0.7645\n",
      "Epoch 485/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4793 - accuracy: 0.7590\n",
      "Epoch 486/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4796 - accuracy: 0.7673\n",
      "Epoch 487/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4810 - accuracy: 0.7645\n",
      "Epoch 488/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4793 - accuracy: 0.7535\n",
      "Epoch 489/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4793 - accuracy: 0.7673\n",
      "Epoch 490/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4802 - accuracy: 0.7590\n",
      "Epoch 491/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4798 - accuracy: 0.7645\n",
      "Epoch 492/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4790 - accuracy: 0.7645\n",
      "Epoch 493/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4811 - accuracy: 0.7645\n",
      "Epoch 494/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4785 - accuracy: 0.7673\n",
      "Epoch 495/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4795 - accuracy: 0.7673\n",
      "Epoch 496/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4801 - accuracy: 0.7618\n",
      "Epoch 497/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4798 - accuracy: 0.7618\n",
      "Epoch 498/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4794 - accuracy: 0.7645\n",
      "Epoch 499/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4791 - accuracy: 0.7673\n",
      "Epoch 500/500\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.4792 - accuracy: 0.7673\n",
      "12/12 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TRUPTI~1\\AppData\\Local\\Temp/ipykernel_25968/2943197496.py:3: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  pred_train = first_model.predict(np.array(x_train).astype(np.int))\n"
     ]
    }
   ],
   "source": [
    "first_model = prep_model([28,50,40,20,1])\n",
    "first_model.fit(np.array(x_train).astype(np.int),np.array(y_train).astype(np.int),epochs=500)\n",
    "pred_train = first_model.predict(np.array(x_train).astype(np.int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351fe798",
   "metadata": {},
   "source": [
    "# converting the predict values to series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8ec2d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = pd.Series([i[0] for i in pred_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "6bf8c7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = [\"small\",\"large\"]\n",
    "pred_train_class = pd.Series([\"small\"]*361)\n",
    "pred_train_class[[i>0.5 for i in pred_train]]= \"large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "015eb3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    264\n",
       "1     97\n",
       "Name: size_category, dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.concat([x_train,y_train],axis=1)\n",
    "train[\"size_category\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6b5ae301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>original_class</th>\n",
       "      <th>large</th>\n",
       "      <th>small</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>large</th>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>small</th>\n",
       "      <td>75</td>\n",
       "      <td>255</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "original_class  large  small\n",
       "row_0                       \n",
       "large              22      9\n",
       "small              75    255"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "train[\"original_class\"] = \"small\"\n",
    "train.loc[train[\"size_category\"]==1,\"original_class\"] = \"large\"\n",
    "train.original_class.value_counts()\n",
    "confusion_matrix(pred_train_class,train[\"original_class\"])\n",
    "np.mean(pred_train_class==pd.Series(train[\"original_class\"]).reset_index(drop=True)) #100%\n",
    "pd.crosstab(pred_train_class,pd.Series(train[\"original_class\"]).reset_index(drop=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d8399eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pred_test = first_model.predict(np.array(x_test))\n",
    "pred_test = pd.Series([i[0] for i in pred_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6e86d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pred_test_class = pd.Series([\"small\"]*156)\n",
    "pred_test_class[[i>0.5 for i in pred_test]] = \"large\"\n",
    "test =pd.concat([x_test,y_test],axis=1)\n",
    "test[\"original_class\"]=\"small\"\n",
    "test.loc[test[\"size_category\"]==1,\"original_class\"] = \"large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "3f1ae62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "small    114\n",
       "large     42\n",
       "Name: original_class, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"original_class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d2a7d8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>original_class</th>\n",
       "      <th>large</th>\n",
       "      <th>small</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>small</th>\n",
       "      <td>42</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "original_class  large  small\n",
       "row_0                       \n",
       "small              42    114"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pred_test_class==pd.Series(test[\"original_class\"]).reset_index(drop=True)) # 85%\n",
    "confusion_matrix(pred_test_class,test[\"original_class\"])\n",
    "pd.crosstab(pred_test_class,pd.Series(test[\"original_class\"]).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758dc84e",
   "metadata": {},
   "source": [
    "# gas turbines dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "45db7788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bf156fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gas = pd.read_csv(\"C:\\\\Users\\\\Trupti Kendre\\\\Downloads\\\\gas_turbines.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "25b80bf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.8594</td>\n",
       "      <td>1007.9</td>\n",
       "      <td>96.799</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>19.663</td>\n",
       "      <td>1059.2</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.70</td>\n",
       "      <td>10.605</td>\n",
       "      <td>3.1547</td>\n",
       "      <td>82.722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.7850</td>\n",
       "      <td>1008.4</td>\n",
       "      <td>97.118</td>\n",
       "      <td>3.4998</td>\n",
       "      <td>19.728</td>\n",
       "      <td>1059.3</td>\n",
       "      <td>550.00</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.598</td>\n",
       "      <td>3.2363</td>\n",
       "      <td>82.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.8977</td>\n",
       "      <td>1008.8</td>\n",
       "      <td>95.939</td>\n",
       "      <td>3.4824</td>\n",
       "      <td>19.779</td>\n",
       "      <td>1059.4</td>\n",
       "      <td>549.87</td>\n",
       "      <td>114.71</td>\n",
       "      <td>10.601</td>\n",
       "      <td>3.2012</td>\n",
       "      <td>82.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.0569</td>\n",
       "      <td>1009.2</td>\n",
       "      <td>95.249</td>\n",
       "      <td>3.4805</td>\n",
       "      <td>19.792</td>\n",
       "      <td>1059.6</td>\n",
       "      <td>549.99</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.606</td>\n",
       "      <td>3.1923</td>\n",
       "      <td>82.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.3978</td>\n",
       "      <td>1009.7</td>\n",
       "      <td>95.150</td>\n",
       "      <td>3.4976</td>\n",
       "      <td>19.765</td>\n",
       "      <td>1059.7</td>\n",
       "      <td>549.98</td>\n",
       "      <td>114.72</td>\n",
       "      <td>10.612</td>\n",
       "      <td>3.2484</td>\n",
       "      <td>82.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15034</th>\n",
       "      <td>9.0301</td>\n",
       "      <td>1005.6</td>\n",
       "      <td>98.460</td>\n",
       "      <td>3.5421</td>\n",
       "      <td>19.164</td>\n",
       "      <td>1049.7</td>\n",
       "      <td>546.21</td>\n",
       "      <td>111.61</td>\n",
       "      <td>10.400</td>\n",
       "      <td>4.5186</td>\n",
       "      <td>79.559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15035</th>\n",
       "      <td>7.8879</td>\n",
       "      <td>1005.9</td>\n",
       "      <td>99.093</td>\n",
       "      <td>3.5059</td>\n",
       "      <td>19.414</td>\n",
       "      <td>1046.3</td>\n",
       "      <td>543.22</td>\n",
       "      <td>111.78</td>\n",
       "      <td>10.433</td>\n",
       "      <td>4.8470</td>\n",
       "      <td>79.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15036</th>\n",
       "      <td>7.2647</td>\n",
       "      <td>1006.3</td>\n",
       "      <td>99.496</td>\n",
       "      <td>3.4770</td>\n",
       "      <td>19.530</td>\n",
       "      <td>1037.7</td>\n",
       "      <td>537.32</td>\n",
       "      <td>110.19</td>\n",
       "      <td>10.483</td>\n",
       "      <td>7.9632</td>\n",
       "      <td>90.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15037</th>\n",
       "      <td>7.0060</td>\n",
       "      <td>1006.8</td>\n",
       "      <td>99.008</td>\n",
       "      <td>3.4486</td>\n",
       "      <td>19.377</td>\n",
       "      <td>1043.2</td>\n",
       "      <td>541.24</td>\n",
       "      <td>110.74</td>\n",
       "      <td>10.533</td>\n",
       "      <td>6.2494</td>\n",
       "      <td>93.227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15038</th>\n",
       "      <td>6.9279</td>\n",
       "      <td>1007.2</td>\n",
       "      <td>97.533</td>\n",
       "      <td>3.4275</td>\n",
       "      <td>19.306</td>\n",
       "      <td>1049.9</td>\n",
       "      <td>545.85</td>\n",
       "      <td>111.58</td>\n",
       "      <td>10.583</td>\n",
       "      <td>4.9816</td>\n",
       "      <td>92.498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15039 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           AT      AP      AH    AFDP    GTEP     TIT     TAT     TEY     CDP  \\\n",
       "0      6.8594  1007.9  96.799  3.5000  19.663  1059.2  550.00  114.70  10.605   \n",
       "1      6.7850  1008.4  97.118  3.4998  19.728  1059.3  550.00  114.72  10.598   \n",
       "2      6.8977  1008.8  95.939  3.4824  19.779  1059.4  549.87  114.71  10.601   \n",
       "3      7.0569  1009.2  95.249  3.4805  19.792  1059.6  549.99  114.72  10.606   \n",
       "4      7.3978  1009.7  95.150  3.4976  19.765  1059.7  549.98  114.72  10.612   \n",
       "...       ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "15034  9.0301  1005.6  98.460  3.5421  19.164  1049.7  546.21  111.61  10.400   \n",
       "15035  7.8879  1005.9  99.093  3.5059  19.414  1046.3  543.22  111.78  10.433   \n",
       "15036  7.2647  1006.3  99.496  3.4770  19.530  1037.7  537.32  110.19  10.483   \n",
       "15037  7.0060  1006.8  99.008  3.4486  19.377  1043.2  541.24  110.74  10.533   \n",
       "15038  6.9279  1007.2  97.533  3.4275  19.306  1049.9  545.85  111.58  10.583   \n",
       "\n",
       "           CO     NOX  \n",
       "0      3.1547  82.722  \n",
       "1      3.2363  82.776  \n",
       "2      3.2012  82.468  \n",
       "3      3.1923  82.670  \n",
       "4      3.2484  82.311  \n",
       "...       ...     ...  \n",
       "15034  4.5186  79.559  \n",
       "15035  4.8470  79.917  \n",
       "15036  7.9632  90.912  \n",
       "15037  6.2494  93.227  \n",
       "15038  4.9816  92.498  \n",
       "\n",
       "[15039 rows x 11 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2e4a51ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15039 entries, 0 to 15038\n",
      "Data columns (total 11 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   AT      15039 non-null  float64\n",
      " 1   AP      15039 non-null  float64\n",
      " 2   AH      15039 non-null  float64\n",
      " 3   AFDP    15039 non-null  float64\n",
      " 4   GTEP    15039 non-null  float64\n",
      " 5   TIT     15039 non-null  float64\n",
      " 6   TAT     15039 non-null  float64\n",
      " 7   TEY     15039 non-null  float64\n",
      " 8   CDP     15039 non-null  float64\n",
      " 9   CO      15039 non-null  float64\n",
      " 10  NOX     15039 non-null  float64\n",
      "dtypes: float64(11)\n",
      "memory usage: 1.3 MB\n"
     ]
    }
   ],
   "source": [
    "gas.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fa1e826f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AT</th>\n",
       "      <th>AP</th>\n",
       "      <th>AH</th>\n",
       "      <th>AFDP</th>\n",
       "      <th>GTEP</th>\n",
       "      <th>TIT</th>\n",
       "      <th>TAT</th>\n",
       "      <th>TEY</th>\n",
       "      <th>CDP</th>\n",
       "      <th>CO</th>\n",
       "      <th>NOX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.00000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "      <td>15039.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>17.764381</td>\n",
       "      <td>1013.19924</td>\n",
       "      <td>79.124174</td>\n",
       "      <td>4.200294</td>\n",
       "      <td>25.419061</td>\n",
       "      <td>1083.798770</td>\n",
       "      <td>545.396183</td>\n",
       "      <td>134.188464</td>\n",
       "      <td>12.102353</td>\n",
       "      <td>1.972499</td>\n",
       "      <td>68.190934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.574323</td>\n",
       "      <td>6.41076</td>\n",
       "      <td>13.793439</td>\n",
       "      <td>0.760197</td>\n",
       "      <td>4.173916</td>\n",
       "      <td>16.527806</td>\n",
       "      <td>7.866803</td>\n",
       "      <td>15.829717</td>\n",
       "      <td>1.103196</td>\n",
       "      <td>2.222206</td>\n",
       "      <td>10.470586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.522300</td>\n",
       "      <td>985.85000</td>\n",
       "      <td>30.344000</td>\n",
       "      <td>2.087400</td>\n",
       "      <td>17.878000</td>\n",
       "      <td>1000.800000</td>\n",
       "      <td>512.450000</td>\n",
       "      <td>100.170000</td>\n",
       "      <td>9.904400</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>27.765000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.408000</td>\n",
       "      <td>1008.90000</td>\n",
       "      <td>69.750000</td>\n",
       "      <td>3.723900</td>\n",
       "      <td>23.294000</td>\n",
       "      <td>1079.600000</td>\n",
       "      <td>542.170000</td>\n",
       "      <td>127.985000</td>\n",
       "      <td>11.622000</td>\n",
       "      <td>0.858055</td>\n",
       "      <td>61.303500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>18.186000</td>\n",
       "      <td>1012.80000</td>\n",
       "      <td>82.266000</td>\n",
       "      <td>4.186200</td>\n",
       "      <td>25.082000</td>\n",
       "      <td>1088.700000</td>\n",
       "      <td>549.890000</td>\n",
       "      <td>133.780000</td>\n",
       "      <td>12.025000</td>\n",
       "      <td>1.390200</td>\n",
       "      <td>66.601000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>23.862500</td>\n",
       "      <td>1016.90000</td>\n",
       "      <td>90.043500</td>\n",
       "      <td>4.550900</td>\n",
       "      <td>27.184000</td>\n",
       "      <td>1096.000000</td>\n",
       "      <td>550.060000</td>\n",
       "      <td>140.895000</td>\n",
       "      <td>12.578000</td>\n",
       "      <td>2.160400</td>\n",
       "      <td>73.935500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>34.929000</td>\n",
       "      <td>1034.20000</td>\n",
       "      <td>100.200000</td>\n",
       "      <td>7.610600</td>\n",
       "      <td>37.402000</td>\n",
       "      <td>1100.800000</td>\n",
       "      <td>550.610000</td>\n",
       "      <td>174.610000</td>\n",
       "      <td>15.081000</td>\n",
       "      <td>44.103000</td>\n",
       "      <td>119.890000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AT           AP            AH          AFDP          GTEP  \\\n",
       "count  15039.000000  15039.00000  15039.000000  15039.000000  15039.000000   \n",
       "mean      17.764381   1013.19924     79.124174      4.200294     25.419061   \n",
       "std        7.574323      6.41076     13.793439      0.760197      4.173916   \n",
       "min        0.522300    985.85000     30.344000      2.087400     17.878000   \n",
       "25%       11.408000   1008.90000     69.750000      3.723900     23.294000   \n",
       "50%       18.186000   1012.80000     82.266000      4.186200     25.082000   \n",
       "75%       23.862500   1016.90000     90.043500      4.550900     27.184000   \n",
       "max       34.929000   1034.20000    100.200000      7.610600     37.402000   \n",
       "\n",
       "                TIT           TAT           TEY           CDP            CO  \\\n",
       "count  15039.000000  15039.000000  15039.000000  15039.000000  15039.000000   \n",
       "mean    1083.798770    545.396183    134.188464     12.102353      1.972499   \n",
       "std       16.527806      7.866803     15.829717      1.103196      2.222206   \n",
       "min     1000.800000    512.450000    100.170000      9.904400      0.000388   \n",
       "25%     1079.600000    542.170000    127.985000     11.622000      0.858055   \n",
       "50%     1088.700000    549.890000    133.780000     12.025000      1.390200   \n",
       "75%     1096.000000    550.060000    140.895000     12.578000      2.160400   \n",
       "max     1100.800000    550.610000    174.610000     15.081000     44.103000   \n",
       "\n",
       "                NOX  \n",
       "count  15039.000000  \n",
       "mean      68.190934  \n",
       "std       10.470586  \n",
       "min       27.765000  \n",
       "25%       61.303500  \n",
       "50%       66.601000  \n",
       "75%       73.935500  \n",
       "max      119.890000  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gas.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b7afe958",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AT      0\n",
       "AP      0\n",
       "AH      0\n",
       "AFDP    0\n",
       "GTEP    0\n",
       "TIT     0\n",
       "TAT     0\n",
       "TEY     0\n",
       "CDP     0\n",
       "CO      0\n",
       "NOX     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gas.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2179d0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Trupti Kendre\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='TEY'>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(gas['TEY'], color='green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "5896553c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = gas.loc[:,['AT', 'AP', 'AH', 'AFDP', 'GTEP', 'TIT', 'TAT', 'CDP', 'CO','NOX']]\n",
    "y= gas.loc[:,['TEY']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "726978e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled = StandardScaler()\n",
    "x = scaled.fit_transform(x)\n",
    "y = scaled.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cedc4b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=10, activation='tanh'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c3788651",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TRUPTI~1\\AppData\\Local\\Temp/ipykernel_25968/3166752672.py:6: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=50, batch_size=100, verbose=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: -0.15 (0.08) MSE\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import InputLayer,Dense\n",
    "estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=50, batch_size=100, verbose=False)\n",
    "kfold = KFold(n_splits=10)\n",
    "results = cross_val_score(estimator, x, y, cv=kfold)\n",
    "print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f1cc9483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.0885243, -1.0719334, -1.0701728, ..., -0.7194831, -0.7831533,\n",
       "       -0.8733213], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.fit(x, y)\n",
    "prediction = estimator.predict(x)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f46c1cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "760a0ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.fit(x_train, y_train)\n",
    "prediction = estimator.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "9f120005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04585219, -0.37263066, -0.5331808 , ..., -0.18176825,\n",
       "        0.10625786,  0.49142358], dtype=float32)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "57428f8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.82623246, -0.43954308, -0.25924569, ...,  0.10039242,\n",
       "        -0.3796304 , -0.69217007],\n",
       "       [ 0.35282087,  0.23279782,  0.80230139, ..., -1.18541222,\n",
       "         0.39149515, -1.09475442],\n",
       "       [ 0.32839008, -0.07135639,  0.25312287, ...,  0.01665304,\n",
       "        -0.00296896, -0.31891741],\n",
       "       ...,\n",
       "       [-0.74071701,  0.37687087,  0.43427425, ...,  1.77157829,\n",
       "        -1.00127821, -0.4818816 ],\n",
       "       [-0.49965786, -0.39151873,  0.64680105, ..., -0.26517949,\n",
       "        -0.48137538,  0.12808615],\n",
       "       [ 0.13151427,  0.32884652,  0.98830762, ..., -1.40331469,\n",
       "         0.13152215, -0.64456466]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = gas.drop(columns = ['TEY'], axis = 1) \n",
    "y = gas.iloc[:,7]\n",
    "from sklearn.preprocessing import scale\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 42)\n",
    "x_train_scaled = scale(x_train)\n",
    "x_test_scaled = scale(x_test)\n",
    "x_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "80727340",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "input_size = len(x.columns)\n",
    "output_size = 1\n",
    "hidden_layer_size = 50\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "                                \n",
    "                               tf.keras.layers.Dense(hidden_layer_size, input_dim = input_size, activation = 'relu'),\n",
    "                               tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "                               tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),\n",
    "                               tf.keras.layers.Dense(hidden_layer_size, activation = 'relu'),     \n",
    "                               tf.keras.layers.Dense(output_size)\n",
    "                             ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "847fab69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "381/381 - 2s - loss: 2980.0088 - mean_squared_error: 2980.0088 - val_loss: 217.7053 - val_mean_squared_error: 217.7053 - 2s/epoch - 6ms/step\n",
      "Epoch 2/100\n",
      "381/381 - 1s - loss: 118.9809 - mean_squared_error: 118.9809 - val_loss: 96.4049 - val_mean_squared_error: 96.4049 - 1s/epoch - 3ms/step\n",
      "Epoch 3/100\n",
      "381/381 - 1s - loss: 54.7118 - mean_squared_error: 54.7118 - val_loss: 43.2850 - val_mean_squared_error: 43.2850 - 1s/epoch - 3ms/step\n",
      "Epoch 4/100\n",
      "381/381 - 1s - loss: 23.6709 - mean_squared_error: 23.6709 - val_loss: 16.5557 - val_mean_squared_error: 16.5557 - 1s/epoch - 3ms/step\n",
      "Epoch 5/100\n",
      "381/381 - 1s - loss: 10.5480 - mean_squared_error: 10.5480 - val_loss: 9.5604 - val_mean_squared_error: 9.5604 - 1s/epoch - 3ms/step\n",
      "Epoch 6/100\n",
      "381/381 - 1s - loss: 5.4250 - mean_squared_error: 5.4250 - val_loss: 3.5569 - val_mean_squared_error: 3.5569 - 979ms/epoch - 3ms/step\n",
      "Epoch 7/100\n",
      "381/381 - 1s - loss: 3.1039 - mean_squared_error: 3.1039 - val_loss: 2.5433 - val_mean_squared_error: 2.5433 - 1s/epoch - 3ms/step\n",
      "Epoch 8/100\n",
      "381/381 - 1s - loss: 2.0463 - mean_squared_error: 2.0463 - val_loss: 1.9935 - val_mean_squared_error: 1.9935 - 1s/epoch - 3ms/step\n",
      "Epoch 9/100\n",
      "381/381 - 1s - loss: 1.5294 - mean_squared_error: 1.5294 - val_loss: 1.5021 - val_mean_squared_error: 1.5021 - 1s/epoch - 3ms/step\n",
      "Epoch 10/100\n",
      "381/381 - 1s - loss: 1.3280 - mean_squared_error: 1.3280 - val_loss: 1.0741 - val_mean_squared_error: 1.0741 - 1s/epoch - 3ms/step\n",
      "Epoch 11/100\n",
      "381/381 - 1s - loss: 1.0714 - mean_squared_error: 1.0714 - val_loss: 1.0952 - val_mean_squared_error: 1.0952 - 1s/epoch - 3ms/step\n",
      "Epoch 12/100\n",
      "381/381 - 1s - loss: 0.9690 - mean_squared_error: 0.9690 - val_loss: 0.8526 - val_mean_squared_error: 0.8526 - 1s/epoch - 3ms/step\n",
      "Epoch 13/100\n",
      "381/381 - 1s - loss: 0.8636 - mean_squared_error: 0.8636 - val_loss: 0.7887 - val_mean_squared_error: 0.7887 - 1s/epoch - 3ms/step\n",
      "Epoch 14/100\n",
      "381/381 - 1s - loss: 0.7919 - mean_squared_error: 0.7919 - val_loss: 0.8814 - val_mean_squared_error: 0.8814 - 1s/epoch - 3ms/step\n",
      "Epoch 15/100\n",
      "381/381 - 1s - loss: 0.9507 - mean_squared_error: 0.9507 - val_loss: 0.6695 - val_mean_squared_error: 0.6695 - 1s/epoch - 3ms/step\n",
      "Epoch 16/100\n",
      "381/381 - 1s - loss: 0.8112 - mean_squared_error: 0.8112 - val_loss: 1.5308 - val_mean_squared_error: 1.5308 - 1s/epoch - 3ms/step\n",
      "Epoch 17/100\n",
      "381/381 - 1s - loss: 0.7298 - mean_squared_error: 0.7298 - val_loss: 0.6458 - val_mean_squared_error: 0.6458 - 1s/epoch - 3ms/step\n",
      "Epoch 18/100\n",
      "381/381 - 1s - loss: 0.8497 - mean_squared_error: 0.8497 - val_loss: 6.6603 - val_mean_squared_error: 6.6603 - 1s/epoch - 3ms/step\n",
      "Epoch 19/100\n",
      "381/381 - 1s - loss: 0.7740 - mean_squared_error: 0.7740 - val_loss: 0.8331 - val_mean_squared_error: 0.8331 - 1s/epoch - 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1caddd23250>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.SGD(learning_rate = 0.03)\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['MeanSquaredError'])\n",
    "num_epochs = 100\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience = 2)\n",
    "model.fit(x_train_scaled, y_train, callbacks = early_stopping, validation_split = 0.1, epochs = num_epochs, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "f64a0f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47/47 [==============================] - 0s 2ms/step - loss: 0.7945 - mean_squared_error: 0.7945\n"
     ]
    }
   ],
   "source": [
    "test_loss, mean_squared_error = model.evaluate(x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "88ee9d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1cadc8240a0>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = model.predict_on_batch(x_test_scaled)\n",
    "plt.scatter(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "be10ec0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>% Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>134.46</td>\n",
       "      <td>135.316711</td>\n",
       "      <td>0.637150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111.88</td>\n",
       "      <td>112.898521</td>\n",
       "      <td>0.910370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>133.72</td>\n",
       "      <td>134.834183</td>\n",
       "      <td>0.833221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>133.79</td>\n",
       "      <td>134.292496</td>\n",
       "      <td>0.375585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110.77</td>\n",
       "      <td>111.670433</td>\n",
       "      <td>0.812885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>132.85</td>\n",
       "      <td>133.685715</td>\n",
       "      <td>0.629066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>125.07</td>\n",
       "      <td>124.046326</td>\n",
       "      <td>0.818481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>160.95</td>\n",
       "      <td>162.137238</td>\n",
       "      <td>0.737644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>133.12</td>\n",
       "      <td>133.658234</td>\n",
       "      <td>0.404322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>111.79</td>\n",
       "      <td>112.941269</td>\n",
       "      <td>1.029850</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1504 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Actual   Predicted   % Error\n",
       "0     134.46  135.316711  0.637150\n",
       "1     111.88  112.898521  0.910370\n",
       "2     133.72  134.834183  0.833221\n",
       "3     133.79  134.292496  0.375585\n",
       "4     110.77  111.670433  0.812885\n",
       "...      ...         ...       ...\n",
       "1499  132.85  133.685715  0.629066\n",
       "1500  125.07  124.046326  0.818481\n",
       "1501  160.95  162.137238  0.737644\n",
       "1502  133.12  133.658234  0.404322\n",
       "1503  111.79  112.941269  1.029850\n",
       "\n",
       "[1504 rows x 3 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df = pd.DataFrame()\n",
    "predictions_df['Actual'] = y_test\n",
    "predictions_df['Predicted'] = predictions\n",
    "predictions_df['% Error'] = abs(predictions_df['Actual'] - predictions_df['Predicted'])/predictions_df['Actual']*100\n",
    "predictions_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6da793f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
